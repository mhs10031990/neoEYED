{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "path = 'C://Users//LENOVO//Desktop//Help_Me!//neobe_code_challenge.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Defines list of functions to be used for Preprocessing and model building and final prediction.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    function to return dataframe for the file provided.\n",
    "\"\"\"\n",
    "    \n",
    "    \n",
    "def read_file(path):\n",
    "    return pd.read_csv(path)\n",
    "    \n",
    "\"\"\"\"\n",
    "    Function to drop timestamp feature and also drop the rows\n",
    "having target label as zero.\n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "def drop_attributes(dataframe):\n",
    "    dataframe = dataframe.drop(['timestamp'], axis=1)\n",
    "    dataframe = dataframe[dataframe['expected_result']!=0]\n",
    "    dataframe.reset_index(inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "\"\"\"\n",
    "    Function to get the respective statistics{mean, max, min, median}\n",
    "of each element(list={each list of column matrix}) of the matrix\n",
    "column(input_attribute)\n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "def matrix_conv(input_attribute, apply_func):\n",
    "    input_attribute = input_attribute.matrix\n",
    "    input_split = input_attribute.split('],')\n",
    "    result = []\n",
    "    for element in input_split:\n",
    "        input_replace = element.replace('[', '')\n",
    "        input_change = input_replace.replace(']', '')\n",
    "        input_update = input_change.split(',')\n",
    "        input_modified = np.array([float(x) for x in input_update])\n",
    "        result.append(apply_func(input_modified))\n",
    "    return result\n",
    "    \n",
    "\"\"\"\n",
    "    Create a single list containing mean of each dimension (total 27)\n",
    "and then segregate them into individuals columns\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def add_stats_col(apply_func, col_initials, dataframe,\n",
    "                  matrix_conv=matrix_conv):\n",
    "    \n",
    "    dataframe[col_initials] = dataframe.apply(matrix_conv, axis=1,\n",
    "                                              apply_func=apply_func)\n",
    "\n",
    "    col_names = [col_initials + str(idx) for idx in range(1, 28)]\n",
    "\n",
    "    return pd.DataFrame(dataframe[col_initials].values.tolist(),\n",
    "                        columns=col_names)\n",
    "\n",
    "\"\"\"\n",
    "    Function to drop all the derived column having very low variance\n",
    "(~0.1) between its mean, median, max and min.\n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "def get_drop_column(column_name, drop_list_var):\n",
    "    drop_col = []\n",
    "    for name in column_name:\n",
    "        for indx in drop_list_var:\n",
    "            drop_col.append(name+str(indx))\n",
    "    return (drop_col)\n",
    "\n",
    "\"\"\"\n",
    "    Function to split the dataframe based on the target class(positive or\n",
    "negative and return the feature and target for each class.\n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "def split_user(dataframe, user_id):\n",
    "    df_user = dataframe[dataframe['user_id'] == user_id]\n",
    "    user_pos = df_user[df_user['expected_result'] == 1]\n",
    "    user_neg = df_user[df_user['expected_result'] == -1]\n",
    "    x_train, y_train = user_pos.iloc[:, :-2], user_pos['expected_result']\n",
    "    x_test, y_test = user_neg.iloc[:, :-2], user_neg['expected_result']\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\"\"\"\n",
    "   Function to apply scaling operation on the input features and return\n",
    "the transformed scaled features.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def scaler_func(x_train, x_test):\n",
    "    col = [x_train.columns]\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(x_train)\n",
    "    return (pd.DataFrame(scaler.transform(x_train), columns=col),\n",
    "            pd.DataFrame(scaler.transform(x_test), columns=col))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Function to fit OneClassSVM on the positive class training set and\n",
    "evaluate the results on negative class testing set and return the\n",
    "calculated metrics.\n",
    "Note - gamma values is choosen after hyper parameter tunning.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def svm_class(user_id, x_train, x_test, y_train, y_test, gamma=0.71):\n",
    "    clf = svm.OneClassSVM(nu=0.15, kernel=\"rbf\", gamma=gamma)\n",
    "    clf.fit(x_train)\n",
    "    y_pred_pos = clf.predict(x_train)\n",
    "    y_pred_neg = clf.predict(x_test)\n",
    "    n_error_train = y_pred_pos[y_pred_pos == -1].size\n",
    "    n_error_test = y_pred_neg[y_pred_neg == 1].size\n",
    "    fap = float(n_error_test*100/y_test.size)\n",
    "    frp = float(n_error_train*100/y_train.size)\n",
    "    return ([user_id, y_train.size, n_error_train,\n",
    "            y_test.size, n_error_test, fap, frp])\n",
    "\n",
    "\"\"\"\n",
    "    Function to call scaler function and pass on the scaled feature\n",
    "to fit into OneClassSVM and fetch the metric for each user.\n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "def get_metric(data_frame, user_ids):\n",
    "    col_name = ['user_id', 'Train size', 'Train Error', 'Test size',\n",
    "                'Test Error', 'False Acceptance %', 'False Rejection %']\n",
    "    data_metric = pd.DataFrame()\n",
    "    metric_list = []\n",
    "    for user_id in user_ids:\n",
    "        x_train, x_test, y_train, y_test = split_user(data_frame, user_id)\n",
    "        scaled_train, scaled_test = scaler_func(x_train, x_test)\n",
    "        metric_list.append(svm_class(user_id, scaled_train, scaled_test,\n",
    "                                     y_train, y_test))\n",
    "\n",
    "    return pd.DataFrame(metric_list, columns=col_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>Train size</th>\n",
       "      <th>Train Error</th>\n",
       "      <th>Test size</th>\n",
       "      <th>Test Error</th>\n",
       "      <th>False Acceptance %</th>\n",
       "      <th>False Rejection %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>87</td>\n",
       "      <td>23</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83</td>\n",
       "      <td>131</td>\n",
       "      <td>28</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84</td>\n",
       "      <td>88</td>\n",
       "      <td>19</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85</td>\n",
       "      <td>101</td>\n",
       "      <td>14</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  Train size  Train Error  Test size  Test Error  \\\n",
       "0       56          87           23         77           0   \n",
       "1       83         131           28         55           0   \n",
       "2       84          88           19         66           1   \n",
       "3       85         101           14         68           0   \n",
       "\n",
       "   False Acceptance %  False Rejection %  \n",
       "0                 0.0               26.0  \n",
       "1                 0.0               21.0  \n",
       "2                 1.0               21.0  \n",
       "3                 0.0               13.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Create a temporary dataframe to capture the variance of each calculated\n",
    "columns (27 * 4 {4 statistics - Mean, Median, Max, Min})\n",
    "\"\"\"\n",
    "'''\n",
    "    Read the input file\n",
    "'''\n",
    "data = read_file(path)\n",
    "\n",
    "\n",
    "'''\n",
    "    Drop the timestamp and rows having labels as zero\n",
    "'''\n",
    "data = drop_attributes(data)\n",
    "\n",
    "\n",
    "'''\n",
    "   Extract the mean, median, max and min of 27 features(Matrix feature) and \n",
    "club the same together \n",
    "'''\n",
    "data_mean = add_stats_col(np.mean, 'Array_mean', data)\n",
    "data_min = add_stats_col(np.min, 'Array_min', data)\n",
    "data_max = add_stats_col(np.max, 'Array_max', data)\n",
    "data_median = add_stats_col(np.median, 'Array_median', data)\n",
    "\n",
    "'''\n",
    "   extract the variance of each derieved columns(above) \n",
    "'''\n",
    "index_value = [idx for idx in range(1, 28)]\n",
    "temp_mean = pd.DataFrame(data_mean.apply(np.var, axis=0), columns=['mean'])\n",
    "temp_min = pd.DataFrame(data_min.apply(np.var, axis=0), columns=['min'])\n",
    "temp_max = pd.DataFrame(data_max.apply(np.var, axis=0), columns=['max'])\n",
    "temp_median = pd.DataFrame(data_median.apply(np.var, axis=0),\n",
    "                           columns=['median'])\n",
    "temp_mean.index = temp_median.index = temp_min.index \\\n",
    "                = temp_max.index = index_value\n",
    "\n",
    "template = pd.concat([temp_mean['mean'],temp_median['median'],\n",
    "                      temp_min['min'],temp_max['max']],axis=1, ignore_index=False)\n",
    "\n",
    "\n",
    "'''\n",
    "   Create the list from Original 27 features having very less variance (<0.1) \\\n",
    "between mean, median, min and max of respective features. \n",
    "'''\n",
    "drop_list_var = template[template.var(axis=1) < 0.1].index.tolist()\n",
    "column_name = ['Array_min','Array_max','Array_mean','Array_median']\n",
    "\n",
    "\n",
    "'''\n",
    "    To check if there exist any collinearity {check specifically for mean}.\n",
    "'''\n",
    "corr_data = data_mean.corr()\n",
    "corr_data[corr_data[corr_data.columns.values]>  0.8]\n",
    "corr_data[corr_data[corr_data.columns.values]< -0.8]\n",
    "\n",
    "'''\n",
    "    Conclusion - Yes but those specified columns are already getting\n",
    "dropped due to low variance across column statistics.\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "   Drop all the columns having very low variance or exhibiting multi-collinearity\n",
    "and append user id and label to remainder dataframe\n",
    "'''\n",
    "final_drop_col = get_drop_column(column_name, drop_list_var)\n",
    "df_club = pd.concat([data_mean, data_median, data_min, data_max],axis=1, ignore_index=False)\n",
    "df_club = df_club.drop(final_drop_col, axis=1)\n",
    "df_club = pd.concat([df_club,data[['expected_result', 'user_id']]] , axis=1)\n",
    "user_ids = np.array(np.unique(data.user_id)).astype(int)\n",
    "\n",
    "'''\n",
    "Call the function which will do below steps for each user:-\n",
    "Extract the data for specified user\n",
    "split the data into two based on positive and negative labels (1, -1)\n",
    "perform feature scaling (standardization) and feature transformation\n",
    "train the complete positive instance on OneClassSVM\n",
    "predict the response on both negative and positive instance\n",
    "collect the error metrics\n",
    "    \n",
    "'''\n",
    "get_metric(df_club, user_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
